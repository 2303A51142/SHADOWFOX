{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/2303A51142/ShadowFox/blob/main/Advance.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Check if GPU is available\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Load the model and tokenizer\n",
        "model_name = \"gpt2\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
        "\n",
        "# Define a function to generate text\n",
        "def generate_text(prompt, max_length=50, temperature=1.0, top_k=50):\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "    outputs = model.generate(\n",
        "        inputs[\"input_ids\"],\n",
        "        max_length=max_length,\n",
        "        temperature=temperature,\n",
        "        top_k=top_k,\n",
        "        do_sample=True,\n",
        "    )\n",
        "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "# Interactive text generation\n",
        "while True:\n",
        "    print(\"\\n--- Text Generation ---\")\n",
        "    user_prompt = input(\"Enter a prompt for the model (or type 'exit' to quit): \").strip()\n",
        "    if user_prompt.lower() == \"exit\":\n",
        "        print(\"Exiting the program. Goodbye!\")\n",
        "        break\n",
        "\n",
        "    max_length = int(input(\"Enter the maximum length for the generated text (default 50): \") or 50)\n",
        "    temperature = float(input(\"Enter the temperature for creativity (default 1.0): \") or 1.0)\n",
        "    top_k = int(input(\"Enter the top-k sampling value (default 50): \") or 50)\n",
        "\n",
        "    generated_text = generate_text(user_prompt, max_length, temperature, top_k)\n",
        "    print(\"\\nGenerated Text:\")\n",
        "    print(generated_text)\n",
        "\n",
        "# Define a pipeline for text generation (optional)\n",
        "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, device=0 if device == \"cuda\" else -1)\n",
        "\n",
        "# Optional: Visualize attention scores\n",
        "def visualize_attention(input_text):\n",
        "    inputs = tokenizer(input_text, return_tensors=\"pt\").to(device)\n",
        "    outputs = model(**inputs, output_attentions=True)\n",
        "    attentions = outputs.attentions  # List of attention tensors from each layer\n",
        "\n",
        "    # Example: Visualize attention from the last layer\n",
        "    layer_attention = attentions[-1][0].detach().cpu().numpy()  # Last layer, batch index 0\n",
        "    avg_attention = layer_attention.mean(axis=0)  # Average over heads\n",
        "\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.heatmap(avg_attention, cmap=\"viridis\")\n",
        "    plt.title(\"Attention Heatmap (Last Layer)\")\n",
        "    plt.xlabel(\"Tokens\")\n",
        "    plt.ylabel(\"Tokens\")\n",
        "    plt.show()\n",
        "\n",
        "# Optional: Visualize user-provided text\n",
        "while True:\n",
        "    print(\"\\n--- Attention Visualization ---\")\n",
        "    user_text = input(\"Enter text to visualize attention (or type 'exit' to quit): \").strip()\n",
        "    if user_text.lower() == \"exit\":\n",
        "        print(\"Exiting the visualization. Goodbye!\")\n",
        "        break\n",
        "\n",
        "    print(\"Generating attention heatmap...\")\n",
        "    visualize_attention(user_text)\n"
      ],
      "metadata": {
        "id": "r7Mdp5J4TtTo",
        "outputId": "8ec8511e-4173-4d92-cdd1-15e93ecd95b0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cpu\n",
            "\n",
            "--- Text Generation ---\n",
            "Enter a prompt for the model (or type 'exit' to quit):  The future of artificial intelligence\n",
            "Enter the maximum length for the generated text (default 50): 60\n",
            "Enter the temperature for creativity (default 1.0): 0.7\n",
            "Enter the top-k sampling value (default 50): 30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Generated Text:\n",
            "The future of artificial intelligence is a question far more uncertain than what it's currently capable of. It seems unlikely that we'll ever be able to master the technology in the near future, given our current reliance on AI, but it's possible that we'll eventually have the technology in our hands.\n",
            "\n",
            "--- Text Generation ---\n",
            "Enter a prompt for the model (or type 'exit' to quit): The future of machine learning \n",
            "Enter the maximum length for the generated text (default 50): 26\n",
            "Enter the temperature for creativity (default 1.0): 0.7\n",
            "Enter the top-k sampling value (default 50): 20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Generated Text:\n",
            "The future of machine learning and AI is in question. As we've written before, machine learning and AI are in their infancy,\n",
            "\n",
            "--- Text Generation ---\n",
            "Enter a prompt for the model (or type 'exit' to quit): exit\n",
            "Exiting the program. Goodbye!\n",
            "\n",
            "--- Attention Visualization ---\n",
            "Enter text to visualize attention (or type 'exit' to quit): exit\n",
            "Exiting the visualization. Goodbye!\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Welcome to Colaboratory",
      "toc_visible": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
