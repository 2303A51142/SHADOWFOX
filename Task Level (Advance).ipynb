{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/2303A51142/ShadowFox/blob/main/Task%20Level%20(Advance).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.Problem Statement: Embark on an AI-driven journey in the realm of\n",
        "natural language processing (NLP) and machine learning (ML) by\n",
        "deploying a Language Model (LM) of your choice. In this project, you\n",
        "are tasked with delving into the intricacies of LM technology, where\n",
        "the selection of the LM is entirely at your discretion. The\n",
        "comprehensive process involves not only implementing the chosen LM\n",
        "but also conducting an in-depth analysis of its performance and\n",
        "capabilities."
      ],
      "metadata": {
        "id": "0pXesssqWNRC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Check if GPU is available\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Load the model and tokenizer\n",
        "model_name = \"gpt2\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
        "\n",
        "# Define a function to generate text\n",
        "def generate_text(prompt, max_length=50, temperature=1.0, top_k=50):\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "    outputs = model.generate(\n",
        "        inputs[\"input_ids\"],\n",
        "        max_length=max_length,\n",
        "        temperature=temperature,\n",
        "        top_k=top_k,\n",
        "        do_sample=True,\n",
        "    )\n",
        "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "# Interactive text generation\n",
        "while True:\n",
        "    print(\"\\n--- Text Generation ---\")\n",
        "    user_prompt = input(\"Enter a prompt for the model (or type 'exit' to quit): \").strip()\n",
        "    if user_prompt.lower() == \"exit\":\n",
        "        print(\"Exiting the program. Goodbye!\")\n",
        "        break\n",
        "\n",
        "    max_length = int(input(\"Enter the maximum length for the generated text (default 50): \") or 50)\n",
        "    temperature = float(input(\"Enter the temperature for creativity (default 1.0): \") or 1.0)\n",
        "    top_k = int(input(\"Enter the top-k sampling value (default 50): \") or 50)\n",
        "\n",
        "    generated_text = generate_text(user_prompt, max_length, temperature, top_k)\n",
        "    print(\"\\nGenerated Text:\")\n",
        "    print(generated_text)\n",
        "\n",
        "# Define a pipeline for text generation (optional)\n",
        "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, device=0 if device == \"cuda\" else -1)\n",
        "\n",
        "# Optional: Visualize attention scores\n",
        "def visualize_attention(input_text):\n",
        "    inputs = tokenizer(input_text, return_tensors=\"pt\").to(device)\n",
        "    outputs = model(**inputs, output_attentions=True)\n",
        "    attentions = outputs.attentions  # List of attention tensors from each layer\n",
        "\n",
        "    # Example: Visualize attention from the last layer\n",
        "    layer_attention = attentions[-1][0].detach().cpu().numpy()  # Last layer, batch index 0\n",
        "    avg_attention = layer_attention.mean(axis=0)  # Average over heads\n",
        "\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.heatmap(avg_attention, cmap=\"viridis\")\n",
        "    plt.title(\"Attention Heatmap (Last Layer)\")\n",
        "    plt.xlabel(\"Tokens\")\n",
        "    plt.ylabel(\"Tokens\")\n",
        "    plt.show()\n",
        "\n",
        "# Optional: Visualize user-provided text\n",
        "while True:\n",
        "    print(\"\\n--- Attention Visualization ---\")\n",
        "    user_text = input(\"Enter text to visualize attention (or type 'exit' to quit): \").strip()\n",
        "    if user_text.lower() == \"exit\":\n",
        "        print(\"Exiting the visualization. Goodbye!\")\n",
        "        break\n",
        "\n",
        "    print(\"Generating attention heatmap...\")\n",
        "    visualize_attention(user_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r7Mdp5J4TtTo",
        "outputId": "de738063-e165-4768-9ace-2778e6f4c3d9"
      },
      "execution_count": 3,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cpu\n",
            "\n",
            "--- Text Generation ---\n",
            "Enter a prompt for the model (or type 'exit' to quit): The future of artifical inteligence\n",
            "Enter the maximum length for the generated text (default 50): 60\n",
            "Enter the temperature for creativity (default 1.0): 0.7\n",
            "Enter the top-k sampling value (default 50): 30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Generated Text:\n",
            "The future of artifical inteligence is the same as that of the human brain. It's not like we can use our brains to solve problems like this, but we can use our brains to solve problems like this.\n",
            "\n",
            "In the future, we'll be able to learn how to solve\n",
            "\n",
            "--- Text Generation ---\n",
            "Enter a prompt for the model (or type 'exit' to quit): exit\n",
            "Exiting the program. Goodbye!\n",
            "\n",
            "--- Attention Visualization ---\n",
            "Enter text to visualize attention (or type 'exit' to quit): exit\n",
            "Exiting the visualization. Goodbye!\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Welcome to Colaboratory",
      "toc_visible": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}